# Complete MLOps Workflow Summary

## Your End-to-End ML Pipeline

This document shows the complete journey from training to batch predictions.

---

## Phase 1: Development (Local Machine)

### Step 1: Train Model
```bash
python train_pipeline.py
```

**What happens:**
- Data Ingestion: Load 21,096 customer records from BigQuery
- Data Validation: Check for missing values, data quality
- Data Transformation: Encode features, apply SMOTE for imbalance
- Model Training: Train 5 models (Logistic Regression, Random Forest, etc.)
- Model Selection: Pick best model (F1-score: 98.5%)
- Model Pusher: Push to GCS + Register in Vertex AI

**Outputs:**
- âœ… Model saved: `final_model/model.pkl`
- âœ… Preprocessor saved: `final_model/preprocessor.pkl`
- âœ… Model registered in Vertex AI Model Registry
- âœ… Model stored in GCS bucket: `gs://mlops-churn-models/`
- âœ… Training logs: `logs/01_25_2026_20_XX_XX.log`

**Time:** ~5 minutes for 21K records

---

## Phase 2: Deployment (Cloud)

### Step 2: Deploy Model to Vertex AI Endpoint
```bash
python deploy_to_vertex_ai.py
```

**What happens:**
- Fetch latest model from GCS
- Create Vertex AI Endpoint (if not exists)
- Deploy model to endpoint
- Test with sample predictions

**Outputs:**
- âœ… Model deployed and serving
- âœ… Endpoint URL for predictions
- âœ… Ready for real-time and batch prediction

**Time:** ~2 minutes

---

## Phase 3: Batch Prediction (Operational)

### Option A: Local Testing (Recommended First)
```bash
python batch_prediction_local.py
```

**What happens:**
- Load test data from local artifacts
- Apply same preprocessing as training
- Make predictions on test set (4,220 records)
- Save results to CSV

**Good for:**
- Testing without deployment
- Validating model performance
- Low latency, no cloud costs

**Outputs:**
- âœ… CSV predictions: `batch_predictions/predictions_local_test_*.csv`
- âœ… Console summary: Churn rate and statistics

**Time:** ~1 minute

---

### Option B: Cloud Batch Prediction (Production)
```bash
python batch_prediction.py
```

**What happens:**
- Load customer data from BigQuery
- Preprocess using trained preprocessor
- Call deployed Vertex AI endpoint
- Save results to BigQuery + CSV

**Good for:**
- Production scoring
- Large datasets (millions of records)
- Integration with business systems

**Outputs:**
- âœ… CSV predictions: `batch_predictions/predictions_*.csv`
- âœ… BigQuery table: `telco_churn_dataset.churn_predictions`
- âœ… Ready for dashboards and reporting

**Time:** Depends on data volume (minutes to hours)

---

## Automated Workflow: CI/CD Pipeline

### Setup GitHub Actions (Optional)
```bash
git push â†’ GitHub detects change
  â†“
.github/workflows/mlops-pipeline.yml triggers
  â†“
Run tests
  â†“
Train model (if tests pass)
  â†“
Push to GCS
  â†“
Deploy to Vertex AI (on main branch)
  â†“
Model is LIVE
```

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA SOURCES                                 â”‚
â”‚  BigQuery (21K customer records) â†’ CSV files â†’ Local artifacts  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  train_pipeline.py         â”‚
        â”‚  - Data Validation         â”‚
        â”‚  - Data Transformation     â”‚
        â”‚  - Model Training          â”‚  (5 min)
        â”‚  - Best Model Selection    â”‚
        â”‚  - GCS + Vertex AI Push    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Vertex AI Model Registry   â”‚
        â”‚ (Model Version Tracking)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ deploy_to_vertex_ai.py     â”‚
        â”‚ - Create endpoint          â”‚  (2 min)
        â”‚ - Deploy model             â”‚
        â”‚ - Ready for serving        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         BATCH PREDICTION                   â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ Path 1: Local Testing                      â”‚
        â”‚ batch_prediction_local.py â†’ CSV predictionsâ”‚
        â”‚                                             â”‚
        â”‚ Path 2: Production (Cloud)                 â”‚
        â”‚ batch_prediction.py                        â”‚
        â”‚ â†’ BigQuery â†’ Vertex AI â†’ Results           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         BUSINESS APPLICATIONS              â”‚
        â”‚ â€¢ Power BI Dashboard                       â”‚
        â”‚ â€¢ Looker Reports                           â”‚
        â”‚ â€¢ Email Alerts (High-risk customers)       â”‚
        â”‚ â€¢ Retention Offers                         â”‚
        â”‚ â€¢ Customer Service Actions                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Overview

### Training & Deployment
| File | Purpose |
|------|---------|
| `train_pipeline.py` | Main training pipeline |
| `deploy_to_vertex_ai.py` | Deploy model to cloud |
| `requirements.txt` | All Python dependencies |
| `cloudbuild.yaml` | CI/CD configuration |

### Batch Prediction
| File | Purpose |
|------|---------|
| `batch_prediction.py` | Cloud batch prediction |
| `batch_prediction_local.py` | Local testing |
| `BATCH_PREDICTION_GUIDE.md` | Detailed guide |

### Models & Data
| Path | Contents |
|------|----------|
| `final_model/` | Trained model + preprocessor |
| `artifacts/` | Training artifacts + CSVs |
| `batch_predictions/` | Output predictions |
| `logs/` | Execution logs |

### Configuration
| File | Purpose |
|------|---------|
| `config/service-account-key.json` | GCP credentials |
| `Networksecurity/` | MLOps components |
| `Data/` | Raw data + ETL scripts |

---

## Typical Weekly Schedule

```
MONDAY 2:00 AM (Cloud Scheduler)
â”œâ”€ Run: train_pipeline.py
â”œâ”€ Retrain model with new week's data
â”œâ”€ Deploy new version to Vertex AI
â””â”€ Update model registry

DAILY 8:00 AM (Cloud Scheduler)
â”œâ”€ Run: batch_prediction.py
â”œâ”€ Score all active customers
â”œâ”€ Save to BigQuery
â””â”€ Trigger Looker refresh

BUSINESS ACTIONS
â”œâ”€ Morning: Dashboard shows 500 high-risk customers
â”œâ”€ Sales: Send retention offers
â”œâ”€ Support: Proactive outreach
â””â”€ Analysis: Measure offer effectiveness
```

---

## Key Metrics Tracked

### Model Performance
- **F1-Score**: 98.5% (how well model predicts churn)
- **Precision**: 97.4% (how many predicted churners actually churn)
- **Recall**: 97.2% (how many actual churners are caught)
- **ROC-AUC**: 0.983 (overall model quality)

### Batch Prediction Output
- Total customers scored
- Predicted churn count
- Churn rate (%)
- Confidence scores by risk tier

### Operational Metrics
- Training time (minutes)
- Prediction latency (seconds)
- Model versions in registry
- Prediction success rate

---

## Troubleshooting

### Training Fails
```bash
# Check logs
tail -f logs/01_25_2026_*.log

# Test data pipeline
python test_data_ingestion.py

# Verify BigQuery connection
python -c "from google.cloud import bigquery; print(bigquery.Client().list_datasets())"
```

### Deployment Fails
```bash
# Check Vertex AI API is enabled
gcloud services enable aiplatform.googleapis.com

# Verify model exists in GCS
gsutil ls gs://mlops-churn-models/

# Check credentials
cat config/service-account-key.json
```

### Batch Prediction Fails
```bash
# Run local version first
python batch_prediction_local.py

# Check endpoint is active
gcloud ai endpoints list --region=us-central1

# Verify BigQuery table exists
bq ls telco_churn_dataset
```

---

## Cost Optimization

| Operation | Cost | Optimization |
|-----------|------|--------------|
| Training (1 run/week) | ~$2 | Use auto-ML or smaller dataset |
| BigQuery queries | ~$0.5/run | Cache results, partition tables |
| Batch prediction | ~$1 | Predict weekly instead of daily |
| Model deployment | ~$5/month | Share endpoints between models |
| Storage (GCS) | <$1 | Archive old model versions |

**Total monthly cost: ~$20-30**

---

## Next Steps

1. âœ… Complete first training run
2. âœ… Deploy model to Vertex AI
3. âœ… Test local batch predictions
4. âœ… Run cloud batch predictions
5. ğŸ†• Set up Cloud Scheduler for automation
6. ğŸ†• Create Looker/Power BI dashboard
7. ğŸ†• Integrate with CRM for actions
8. ğŸ†• Monitor model performance over time

---

## Support & Resources

- **Training logs**: `logs/` directory
- **Error troubleshooting**: See logs + `CI_CD_SETUP.md`
- **Batch prediction guide**: `BATCH_PREDICTION_GUIDE.md`
- **Model details**: `Networksecurity/Components/model_trainer.py`
- **Data pipeline**: `Networksecurity/Components/data_ingestion.py`

---

**Last Updated**: January 25, 2026
**Status**: âœ… Ready for Production
